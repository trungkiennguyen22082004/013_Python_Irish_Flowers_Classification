{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIẢI QUYẾT BÀI TOÁN PHÂN LOẠI HOA IRIS SỬ DỤNG MẠNG LƯỚI THẦN KINH SÂU\n",
    "\n",
    "## GIỚI THIỆU\n",
    "\n",
    "Trong bài viết này, chúng ta hãy cùng tìm hiểu về những điều cơ bản nhất về Mạng lưới thần kinh sâu (Deep Neural Network), cũng như một trong những bài toán kinh điển sử dụng nó - Phân loại hoa Iris (IRIS Flowers Classification)\n",
    "\n",
    "**Mục tiêu của bài viết:**\n",
    "\n",
    "- Giới thiệu về các thành phần mạng lưới thần kinh sâu\n",
    "- Xây dựng một mạng lưới thần kinh sâu từ A đến Z\n",
    "- Sự dụng mạng lưới thần kinh sâu đã tạo để giải quyết bài toán kinh điển Phân loại hoa Iris\n",
    "\n",
    "**Chú thích**\n",
    "- Ký hiệu các chứ cái in hoa là các ma trận: $X$, $A$, $Z$, $W$\n",
    "- Ký hiệu các chữ cái in thường là các vector với kích thước (<số tự nhiên>, 1): $x$, $a$, $z$, $w$, $b$\n",
    "- Ký hiệu $[l]$ đại diện cho lớp thứ $l^{th}$\n",
    "    - Ký hiệu $L$ là số lớp của mạng thần kinh (Số lớp ẩn + 1)\n",
    "    - Ký hiệu $n^{l}$ là số neuron trong lớp thứ l\n",
    "    - Ký hiệu $a^{[L]}$ là hàm kích hoạt (activation function) của lớp thứ $l^{th}$\n",
    "- Ký hiệu $(i)$ đại diện cho thí dụ thứ $i^{th}$\n",
    "    - Ký hiệu $n^(i)$ là ví dụ thứ $i^{th}$ của tập training\n",
    "- Ký hiệu $i$ đại diện cho vị trí thứ $i^{th}$ của 1 vector\n",
    "    - Ký hiệu $a^{[l]}_i$ là phần tử thứ $i^{th}$ của hàm kích hoạt của lớp thứ $l^{th}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MỤC LỤC\n",
    "- [1 - Tổng quan về Mạng lưới thần kinh sâu](#1)\n",
    "- [2 - Giới thiệu về bài toán Phân loại hoa IRIS](#2)\n",
    "- [3 - Xây dựng mạng lưới thần kinh sâu](#3)\n",
    "- [4 - Xử lý đầu vào](#4)\n",
    "- [5 - Training và Testing](#5)\n",
    "- [6 - Kết luận](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "### 1 - Tổng quan về Mạng lưới thần kinh sâu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Về cơ bản, việc xây dựng các loại mạng lưới thần kinh sâu đều được lấy cảm hứng từ cấu trúc và chức năng của bộ não con người. Chúng được phát triển nhằm mục đích nhận diện các mẫu dữ liệu từ thế giới bên ngoài, học hỏi từ kinh nghiệm, từ đó giúp xử lý nhiều vấn đề từ đơn giản đến phức tạp. Hiện nay, các lĩnh vực phổ biến có áp dụng Mạng lưới thần kinh sâu bao gồm Thị giác máy tính (Computer Vision), Xử lý ngôn ngữ tự nhiên (Natural Language Processing), Phát hiện bất thường (Anomaly Dectection), Nhận diện giọng nói (Speech Recognition), .... Kể từ đây, bài viết sẽ chỉ nhắc đến khái niệm Mạng lưới thần kinh tiêu chuẩn (Standard Neural Network), vốn là mô hình mạng lưới thần kinh sâu cơ bản nhất.\n",
    "\n",
    "Một mạng lưới thần kinh sâu đơn giản có cấu trúc như sau:\n",
    "- Lớp đầu vào (Input layer): Lớp này có chức năng nhận dữ liệu ban đầu, thường là các số liệu. Các dạng đầu vào khác như hình ảnh, âm thanh, văn bản có thể được chuyển đổi thành như ma trận số liệu theo những phương pháp khác nhau.\n",
    "- (Các) lớp ẩn (Hidden layer(s)): (Các) lớp ẩn là những lớp trung gian trong mạng lưới thần kinh sâu, đóng vai trò chuyển tiếp giữa lớp đầu vào và lớp đầu ra. Mỗi lớp ẩn bao gồm một số lượng nhất định các neurons, có nhiệm vụ xử lý và phân tích dữ liệu bằng cách sử dụng một loạt các công thức toán học, đồng thời trích xuất những đặc trưng của dữ liệu đầu vào.\n",
    "- Lớp đầu ra (Output layer): Lớp này có chức năng đưa ra các đầu ra cuối cùng (giá trị dự đoán) bằng cách sử dụng các đặc trưng đã được trích xuất. Tùy thuộc vào kiểu dữ liệu mong muốn, lớp này có thể có một hoặc nhiều neurons. Mục tiêu của mạng lưới thần kinh sâu là tối thiểu hóa sai số giữa giá trị dự đoán và giá trị thực tế.\n",
    "\n",
    "Mỗi một lớp ẩn trong mạng sẽ có các thành phần sau đây:\n",
    "- Trọng số - Weight ($W$): Kết nối giữa một lớp ẩn đến một lớp ẩn kế tiếp được điều chỉnh bởi các trọng số của lớp ẩn đó. Chúng thể hiện mức độ quan trọng của đầu vào và đầu ra của các neurons.\n",
    "- Ngưỡng - Bias($b$): Là các giá trị độc lập dùng để điều chỉnh độ lệch của đầu ra của các neurons.\n",
    "- Hàm kích hoạt - Activation function: Đây là một hàm phi tuyến tính như ReLU, Sigmoid, Softmax, .... Nếu các lớp ẩn trong một mạng lưới thần kinh sâu chỉ sử dụng các trọng số và ngưỡng để tính toán thì mạng đó chỉ là một phương trình tuyến tính khổng lồ (linear function), và nó hoàn toàn không thể biển diễn được các vấn đề phức tạp. Do vậy, các hàm kích hoạt phi tuyến tính được áp dụng nhằm mô hình hóa các mối liên hệ phức tạp hơn.\n",
    "Từ đó, với $X$ là giá trị đầu vào, $Z$ là giá trị đầu ra của mỗi lớp ẩn, ta có thể xây dựng phương trình lớp ẩn như sau:\n",
    "$$ Z = activationFunction(WX + b)$$\n",
    "\n",
    "**Lan truyền tiến (Forward Propagation)**\n",
    "- Trong quá trình này, giá trị đầu vào sẽ được sử dụng để tính toán giá trị đầu ra của các lớp trung gian, cái mà tiếp tục được sử dụng để tính toán giá trị đầu ra của các lớp trung gian tiếp theo trong mạng. Cuối cùng, giá trị đầu ra của lớp đầu ra được gọi là giá trị dự đoán.\n",
    "    - Để đo sự khác biệt giữa giá trị dự đoán và giá trị thực tế, ta sẽ sử dụng hàm mất mát (Loss function). Việc lựa chọn hàm mất mát sẽ tùy thuộc vào bài toán mà chúng ta cần giải quyết.\n",
    "\n",
    "**Lan truyền ngược (Backward Propagation)**\n",
    "- Ở đây, (các) sai số giữa giá trị dự đoán và giá trị thực tế được điều chỉnh các tham số của từng lớp ẩn (trọng số và ngưỡng), nhằm tối thiểu hóa sai số.\n",
    "    - Đầu tiên, tính toán giá trị đầu hàm của hàm mất mát theo giá trị đầu ra dự đoán của mạng.\n",
    "    - Từ đó, sử dụng dây chuyền trong tích phân (Chain rule), để tính giá trị đạo hàm của hàm mất mát theo tham số của từng lớp ẩn\n",
    "    - Sau khi đã tính toán được các giá trị đạo hàm này, chúng ta sử dụng phương pháp suy giảm độ dốc (Gradient Descent), để cập nhật giá trị của các tham số\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "### 2 - Giới thiệu về bài toán Phân loại hoa IRIS\n",
    "\n",
    "Bài toán phân loại hoa IRIS là một trong những bài toán kinh điển trong lĩnh vực học máy (Machine Learning). Mục tiêu của bài toán là phân loại hoa vào một trong ba loại: Iris setosa, Iris versicolor và Iris virginica, dựa trên các đặc trưng của chúng.\n",
    "\n",
    "Các đặc trưng của hoa Iris được thu thập và công bố lần đầu tiên vào năm 1936, bởi nhà khoa học người Anh Ronald Fisher. Chúng bao gồm: \n",
    "- Chiều dài đài hoa (sepal length)\n",
    "- Chiều rộng đài hoa (sepal width)\n",
    "- Chiều dài cánh hoa (petal length)\n",
    "- Chiều rộng cánh hoa (petal width)\n",
    "\n",
    "Dữ liệu về hoa được sử dụng được tải xuống từ [Kaggle](https://www.kaggle.com/datasets/arshid/iris-flower-dataset), bao gồm 150 mẫu về hoa iris, với 50 mẫu từ mỗi loài trong số ba loài trên.\n",
    "\n",
    "Bài toán này là một bài toán phân phối xác suất với 3 giá trị, do vậy hàm kích hoạt cho lớp đầu ra được sử dụng sẽ là hàm Softmax:\n",
    "$$\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{L} e^{z_j}}$$\n",
    "\n",
    "trong đó:\n",
    "- $z$ vector đầu ra từ lớp ẩn của mạng\n",
    "- $e$ cơ số của logarit tự nhiên (số Euler)\n",
    "\n",
    "Đối với các lớp ẩn khác, các hàm kích hoạt khác như ReLu, Sigmoid có thể được sử dụng. Tuy nhiên, để tránh rắc rối trong việc thực hiện, ta sẽ áp dụng hàm Softmax cho tất cả các lớp ẩn trong mạng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "### 3 - Xây dựng mạng lưới thần kinh sâu\n",
    "\n",
    "Để xây dựng mạng lưới thần kinh sâu một cách dễ dàng và dễ hiểu, chúng ta sẽ áp dụng các nguyên tắc trong lập trình hướng đối tượng (Object-oriented programming). Theo đó, mỗi lớp trong mạng sẽ biểu hiện bởi class `StandardLayer`. Lớp này sẽ bao gồm một lớp tuyến tính, và một lớp kích hoạt Sigmoid, được biểu hiện bằng 2 class tương ứng `Linear` và `Sigmoid`. Tất cả các class này đều được thực hiện các phương thức tương ứng cho quá trình lan truyền tiến và ngược, đó là `forward` và `backward`. Ngoài ra, hàm `backward` của class `Linear` cũng sẽ được thực hiện phương pháp suy giảm độ dốc để cập nhật các tham số. \n",
    "\n",
    "Sau đó, một class sẽ biểu diễn cho mạng, gọi là `NeuralNetwork`. Class này sẽ có các phương thức như sau:\n",
    "- `predict` dùng cho quá trình training và testing mô hình\n",
    "- `computeCost` dùng để tính hàm mất mát sau quá trình lan truyền tiến\n",
    "- `computeCostGradient` dùng để tính đạo hàm của hàm mất mát trên giá trị dự đoán (đầu ra) của mạng, để phục vụ cho quá trình lan truyền ngược\n",
    "\n",
    "<a name='3-1'></a>\n",
    "#### 3.1 - Thư viện được sử dụng\n",
    "\n",
    "Thư viện [numpy](www.numpy.org) sẽ được sử dụng, dùng để thực hiện các tính toán giữa các ma trận trong cả hai quá trình lan truyền tiến và ngược của ma trận. Phiên bản numpy 1.26.4 được khuyến khích sử dụng trong quá trình thực hiện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "#### 3.2 - Xây dựng lớp tuyến tính\n",
    "\n",
    "<a name='3-2-1'></a>\n",
    "##### 3.2.1 - Khởi tạo lớp tuyến tính\n",
    "\n",
    "**Tham số cho lớp tuyến tính**:\n",
    "Mỗi một lớp sẽ có một số lượng các inputs (cũng là số neurons của lớp trước đó), và một số lượng các outputs (cũng là số neurons của lớp hiện tại). Chúng ta cần khởi tạo các tham số của một lớp, đó là các trọng số ($W$), và các ngưỡng ($b$) dựa vào các dữ liệu trên.\n",
    "- Đối với các trọng số, ta sẽ khởi tạo một ma trận ngẫu nhiên có kích thước (<số neurons lớp trước>, <số neurons lớp hiện tại>), sử dụng hàm `random.rand` của thư viện `numpy`. \n",
    "- Đối với các ngưỡng, độ lệch có thể bằng 0 vào thời điểm ban đầu, nên ta sẽ khởi tạo một vector với các phần tử bằng 0 với kích thước (<số neurons lớp hiện tại>, 1), sử dụng hàm `zeros` của thư viện `numpy`\n",
    "\n",
    "**Tốc độ học**:\n",
    "Tốc độ học cũng là một thuộc tính cần thiết của lớp tuyến tính, khi mà nó sẽ được sử dụng trong quá trình cập nhất tham số với Suy giảm độ dốc.\n",
    "Nó sẽ là một tham số khi ta khởi tạo đối tượng `Linear` cho lớp tuyến tính, nên ta chỉ đơn giản là gán giá trị cho thuộc tính `self.learningRate`.\n",
    "\n",
    "**Bộ nhớ tạm**:\n",
    "Một thuộc tính khác sẽ được khởi tạo để lưu trữ đầu vào của lớp, các tham số trước khi cập nhật, gọi là `self.cache`, sẽ được sử dụng cho quá trình lan truyền ngược của lớp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \n",
    "    def __init__(self, inputDimension, outputDimension, learningRate=3e-3):\n",
    "        \n",
    "        # inputDimension means the number of units in the previous hidden layer, or the input size of the input layer (if that is the previous layer)\n",
    "        # outputDimension also means the number of units in the current layer\n",
    "        \n",
    "        self.W = np.random.rand(outputDimension, inputDimension) * 0.01\n",
    "        self.b = np.zeros((outputDimension, 1))\n",
    "        \n",
    "        self.cache = None\n",
    "        \n",
    "        self.learingRate = learningRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2-2'></a>\n",
    "##### 3.2.2 - Xây dựng hàm cho quá trình Lan truyền tiến của lớp tuyến tính\n",
    "\n",
    "Ta chỉ đơn giản áp dùng hàm tuyến tính cho quá trình lan truyền tiến của lớp:\n",
    "$$ Z = WA + b$$\n",
    "\n",
    "Ta cần xây dựng phép nhân ma trận để tính $Z$\n",
    "\n",
    "Ví dụ, cho:\n",
    "- Số lượng mẫu của đầu vào: 4\n",
    "- Số lượng neurons của lớp trước: 3\n",
    "- Số lượng neurons của lớp hiện tại: 2\n",
    "- $A$ với kích thước (3, 4):\n",
    "$$ A = \\begin{bmatrix}\n",
    "    a_{00} & a_{01} & a_{02} & a_{03} \\\\\n",
    "    a_{10} & a_{11} & a_{12} & a_{13} \\\\\n",
    "    a_{20} & a_{21} & a_{22} & a_{23}\n",
    "    \\end{bmatrix}\\;\\;\\;\n",
    "$$\n",
    "- $W$ với kích thước (3, 4):\n",
    "$$\n",
    "   W = \\begin{bmatrix} \n",
    "    w_{00} & w_{01} & w_{02} \\\\\n",
    "    w_{10} & w_{11} & w_{12} \n",
    "    \\end{bmatrix}\\;\\;\\;\n",
    "$$\n",
    "\n",
    "- $b$ với kích thước (2, 1):\n",
    "$$\n",
    "   b = \\begin{bmatrix} \n",
    "    b_{0} \\\\\n",
    "    b_{1} \n",
    "    \\end{bmatrix}\\;\\;\\;\n",
    "$$\n",
    "\n",
    "$Z$ sẽ được tính như sau:\n",
    "$$\n",
    "   Z = \\begin{bmatrix} \n",
    "    w_{00} * a_{00} + w_{01} * a_{10} + w_{02} * a_{20} + b_{0} & w_{00} * a_{01} + w_{01} * a_{11} + w_{02} * a_{21} + b_{0} & w_{00} * a_{02} + w_{01} * a_{12} + w_{02} * a_{22} + b_{0} & w_{00} * a_{03} + w_{01} * a_{13} + w_{02} * a_{23} + b_0 \\\\\n",
    "    w_{10} * a_{00} + w_{11} * a_{10} + w_{12} * a_{20} + b_{1} & w_{10} * a_{01} + w_{11} * a_{11} + w_{12} * a_{21} + b_{1} & w_{10} * a_{02} + w_{11} * a_{12} + w_{12} * a_{22} + b_{1} & w_{10} * a_{03} + w_{11} * a_{13} + w_{12} * a_{23} + b_1\n",
    "    \\end{bmatrix}\\;\\;\\;\n",
    "$$\n",
    "\n",
    "Để thực hiện phép nhân ma trận, ta có thể tính bằng cách sử dụng 2 vòng lặp `for`, hoặc sử dụng hàm `dot` của thư viện `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \n",
    "        # A is the output from the previous hidden layer with the shape of (<number of units in that previous layer>, <number of examples>)\n",
    "        \n",
    "        # Cache is stored for computing the backward pass efficiently\n",
    "        self.cache = (A, self.W, self.b)\n",
    "        \n",
    "        # Calculate properly:\n",
    "        (previousLayerDimension, numberOfExamples) = A.shape\n",
    "        \n",
    "        currentLayerDimension = self.W.shape[0]\n",
    "        \n",
    "        Z = np.zeros((currentLayerDimension, numberOfExamples))\n",
    "        \n",
    "        for i in range(currentLayerDimension):\n",
    "            for j in range(numberOfExamples):\n",
    "                dot = 0\n",
    "                for k in range(previousLayerDimension):\n",
    "                    dot += A[k, j] * self.W[i, k]\n",
    "                    \n",
    "                # Add the bias\n",
    "                Z[i, j] = dot + self.b[i]\n",
    "        \n",
    "        # Calculate simply using numpy matrix multiplication:\n",
    "        # Z = np.dot(self.W, A) + self.b\n",
    "        \n",
    "        return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2-3'></a>\n",
    "##### 3.2.3 - Xây dựng hàm cho quá trình Lan truyền ngược của lớp tuyến tính\n",
    "\n",
    "Ý tưởng của Mạng thần kinh sâu là sử dụng phương pháp suy giảm độ sâu, dựa trên giá trị đạo hàm của hàm mất mát theo các giá trị tham số của mỗi lớp trong mạng, qua đó cập nhật những tham số đó để Mô hình trở nên đúng hơn. Đầu tiên, chúng ta sẽ tính giá trị đạo hàm của hàm mất mát theo các giá trị tham số của lớp.\n",
    "\n",
    "##### a. Tính giá trị đạo hàm của hàm mất mát theo các giá trị tham số của lớp.\n",
    "\n",
    "Theo lớp tuyến tính, ta đã đề cập công thức sau ở phần 3.2.2: $$Z = W A + b\\tag{3.2.3-a}$$\n",
    "\n",
    "Cho $deltaZ$, đại lượng mà giả sử đã được tính toán trước đó, là giá trị đạo hàm của hàm mất mát theo giá trị đầu ra của lớp. Nói cách khác, $deltaZ = \\frac{\\partial \\mathcal{L} }{\\partial Z}$. Ta cần tính:\n",
    "- $deltaW = \\frac{\\partial \\mathcal{L} }{\\partial W}$ là giá trị đạo hàm của hàm mất mát theo trọng số $W$ của lớp.\n",
    "- $deltab = \\frac{\\partial \\mathcal{L} }{\\partial b}$ là giá trị đạo hàm của hàm mất mát theo ngưỡng $b$ của lớp.\n",
    "- Ta cũng sẽ tính giá trị đạo hàm của hàm mất mát theo đầu vào $A$ của lớp, ký hiệu là $deltaA = \\frac{\\partial \\mathcal{L} }{\\partial A}$, nhằm lưu trữ vào bộ nhớ tạm để có thể sử dụng cho các bước kế tiếp.\n",
    "\n",
    "Để tính các giá trị đạo hàm trên, ta sẽ làm quen với một quy tắc rất hữu ích trong tích phân, đó là quy tắc dây chuyền (Chain rule):\n",
    "\n",
    "Cho hai hàm số $f$ và $g$, thì đạo hàm của hàm hợp $f(g(x))$ theo x được tính như sau: $$\\frac{d}{dx}[f(g(x))] = f'(g(x)) \\cdot g'(x)$$\n",
    "Đơn giản hơn, ta có thể viết lại công thức trên như sau: \n",
    "$$\\frac{\\partial f }{\\partial x} = \\frac{\\partial f }{\\partial g} \\frac{\\partial g }{\\partial x}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "***Bây giờ ta sẽ áp dụng luật này để tính các giá trị đạo hàm đã liệt kê, bắt đầu với $deltaW$:***\n",
    "$$\\frac{\\partial \\mathcal{L} }{\\partial W} = \\frac{\\partial \\mathcal{L} }{\\partial Z} \\frac{\\partial Z }{\\partial W} \\tag{3.2.3-b}$$\n",
    "\n",
    "Mặt khác, từ công thức (3.2.3-a), ta có thể tính giá trị đạo hàm của đầu ra của lớp theo trọng số tương ứng ($\\frac{\\partial Z }{\\partial W}$) như sau:\n",
    "$$\\frac{\\partial Z }{\\partial W} = \\frac{\\partial (W A + b) }{\\partial W} = A \\tag{3.2.3-c}$$\n",
    "\n",
    "Từ (3.2.3-c), ta có thể biến đổi (3.2.3-b) thành:\n",
    "$$deltaW = deltaZ * A\\tag{3.2.3-d}$$\n",
    "\n",
    "***Tương tự, đối với giá trị $deltab$, ta có:***\n",
    "$$ \\frac{\\partial Z }{\\partial b} = \\frac{\\partial (W A + b) }{\\partial b} = 1 $$\n",
    "$$ \\frac{\\partial \\mathcal{L} }{\\partial b} = \\frac{\\partial \\mathcal{L} }{\\partial Z} \\frac{\\partial Z }{\\partial b} <=> deltab = deltaZ \\tag{3.2.3-e}$$\n",
    "\n",
    "***Và với $deltaA$:***\n",
    "\n",
    "$$ \\frac{\\partial Z }{\\partial A} = \\frac{\\partial (W A + b) }{\\partial A} = W $$\n",
    "$$ \\frac{\\partial \\mathcal{L} }{\\partial A} = \\frac{\\partial \\mathcal{L} }{\\partial Z} \\frac{\\partial Z }{\\partial A} <=> deltaA = deltaZ * W \\tag{3.2.3-f}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backward(self, deltaZ):\n",
    "        \n",
    "        # Get the previous input\n",
    "        A = self.cache[0]\n",
    "        \n",
    "        # deltaW is the gradient of the cost with respect to the weights W of the current linear layer (dZ / dW)\n",
    "        deltaW = np.dot(deltaZ, A.transpose())\n",
    "        \n",
    "        # deltab is the gradient of the cost with respect to the biases b of the current linear layer (dZ / db)\n",
    "        deltab = np.sum(deltaZ, axis=1)\n",
    "        \n",
    "        # deltaA is the gradient of the cost with respect to the input of the current linear layer (which is also the output of the previous layer)\n",
    "        deltaA = np.dot(self.W.transpose(), deltaZ)\n",
    "        \n",
    "        # Update the weights and biases\n",
    "        self.updateParamaters(deltaW=deltaW, deltab=deltab)\n",
    "        \n",
    "        return (deltaA, deltaW, deltab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Cập nhật các them số của lớp bằng Suy giảm độ dốc\n",
    "\n",
    "**Giới thiệu về Suy giảm độ dốc**\n",
    "\n",
    "<div style=\"text-align: center;\"><img src=\"images/gradient_descent_illustration_1.png\" style=\"width:585px;height:555px;\"></div>\n",
    "<caption><center><b>Ảnh 3.2.3-a. Một đồ thị hàm số điển hình. Được lấy từ <a>https://machinelearningcoban.com/2017/01/12/gradientdescent/#2-gradient-descent-cho-h%c3%a0m-1-bi%e1%ba%bfn</a></b></center></caption><br>\n",
    "\n",
    "Trong ảnh trên, điểm màu xanh lục $x*$ là một cực tiểu của hàm số f, và cũng là điểm làm cho hàm số này đạt giá trị nhỏ nhất. Tại điểm này, giá trị đạo hàm của hàm số đã cho sẽ bị triệu tiêu (bằng 0). Giá trị đạo hàm của các điểm lân cận bên trái của điểm này bé hơn hoặc bằng 0, và giá trị đạo hàm của các điểm lân cận bên phải của nó sẽ lớn hơn hoặc bằng 0.\n",
    "\n",
    "<br>\n",
    "\n",
    "Từ đó, ta có thể nhận thấy, x càng xa $x*$ về bên phải, thì giá trị đạo hàm tương ứng càng lớn hơn 0, khi đó ta cần **giảm** giá trị x một đoạn tỉ lệ thuận với giá trị đạo hàm của nó:\n",
    "$$ x_{i+1} = x_{i} - f'(x_{i}) $$\n",
    "\n",
    "Ngược lại, khi x càng xa $x*$ về bên trái, thì giá trị đạo hàm tương ứng càng bé hơn 0, khi đó ta cần **tăng** giá trị x một đoạn tỉ lệ thuận với giá trị đạo hàm của nó:\n",
    "$$ x_{i+1} = x_{i} - f'(x_{i}) $$\n",
    "\n",
    "Áp dụng vào bài toán, ta có thể cập nhật các tham số của lớp tuyến tính như sau:\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } deltaW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } deltab^{[l]} \\tag{17}$$\n",
    "\n",
    "với \n",
    "- $deltaW$ và $deltab$ là các giá trị đạo hàm của hàm mất mát theo các tham số đã được tính ở (3.2.3-d) và (3.2.3-e)\n",
    "- $\\alpha$ là tốc độ học\n",
    "\n",
    "Ta sẽ chọn một hàm mất mát sao cho \"dấu\" của các giá trị đạo hàm sẽ phù hợp để tìm giá trị cực tiểu, để ta chỉ cần chọn tốc độ học mà không cần phải xét dấu của chúng, đó là Binary Cross-Entropy Loss. Ta sẽ nói cụ thể ở các phần sau, nhìn chung, ở đây ta chỉ cần một giá trị $\\alpha$ hợp lý. Nếu giá trị này quá nhỏ thì bước điều chỉnh của các tham số sẽ nhỏ, rất đến tốn nhiều thời gian để cập nhật các tham số đến với các giá trị gần đúng. Tuy nhiên, nếu giá trị này quá lớn thì ta sẽ bị \"bỏ lỡ\" giá trị cực tiểu của hàm mất mát, dẫn tới sự sai lệch của mô hình. Sau khi thực hiện một vài thử nghiệm, tôi đã lấy giá trị $3^{e}-3$ cho tốc độ học.\n",
    "\n",
    "Khi thực hiện, ta có thể nhân một cách đơn giản giữa một số thực với một ma trận numpy, hoặc sử dụng những vòng lặp for để hiểu bản chất hơn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def updateParamaters(self, deltaW, deltab):\n",
    "        \n",
    "        # self.W -= self.learingRate * deltaW\n",
    "        # self.b -= self.learingRate * deltab\n",
    "        \n",
    "        for i in range(0, self.W.shape[0]):\n",
    "            for j in range(0, self.W.shape[1]):\n",
    "                self.W[i][j] -= self.learingRate * deltaW[i][j]\n",
    "        for i in range(0, len(self.b)):\n",
    "            self.b[i] -= self.learingRate * deltab[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "### 4 - Xử lý đầu vào"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "### 5 - Training và Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "### 6 - Kết luận"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
